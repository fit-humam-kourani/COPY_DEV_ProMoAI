Model,Total Average,Up_to_12,13_and_Higher
ground truth,0.98422327,0.983951851,0.984727335
o1-preview,0.918281977,0.934598587,0.8879797
claude-3-5-sonnet-20240620,0.93134829,0.93302703,0.92823063
o1-mini,0.90874262,0.932964543,0.86375905
gemini-1.5-pro-002-IT1,0.874262709,0.88597951,0.852502936
gemini-1.5-pro-002-IT2,0.88396123,0.882583391,0.886520075
Llama-3.1-405B-Instruct,0.856924494,0.871348314,0.830137401
Llama-3.1-Nemotron-70B-Instruct,0.833543322,0.864611912,0.775844512
gemini-1.5-pro-002-IT3,0.865330468,0.859063063,0.876969934
gemini-1.5-pro-002-IT4,0.861565061,0.855593506,0.872655093
mistral-large-2407,0.7758067,0.843997864,0.649165967
Qwen2.5-72B-Instruct,0.802630484,0.820317967,0.769782301
Llama-3.2-90B-Vision-Instruct,0.803826375,0.815921306,0.781364361
open-mixtral-8x22b,0.720249771,0.807871136,0.557524379
codestral-2405,0.731909419,0.779211723,0.644062283
gpt-4o,0.755074559,0.77699313,0.714368642
gpt-4,0.755682246,0.770898314,0.727423833
WizardLM-2-8x22B,0.732876181,0.745604517,0.709237842
gemini-1.5-flash-002,0.7268155,0.73364347,0.714134983
gpt-4o-mini,0.736372416,0.720597195,0.765669254
